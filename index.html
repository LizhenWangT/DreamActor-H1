% CVPR 2024 Paper Template; see https://github.com/cvpr-org/author-kit

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Import additional packages in the preamble file, before hyperref
\input{preamble}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g. with the file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,citecolor=cvprblue]{hyperref}
% begin self
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{bm}

% end self


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{} % *** Enter the Paper ID here
\def\confName{}
\def\confYear{2025}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{
DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers
}

%%%%%%%%% AUTHORS - PLEASE UPDATE
\author{
  $\text{Lizhen Wang}^1$, $\text{Zhurong Xia}^1$\textsuperscript{*}, $\text{Tianshu Hu}^1$, 
  $\text{Pengrui Wang}^1$, $\text{Pengfei Wei}^1$, \\
  $\text{Zerong Zheng}^1$, $\text{Ming Zhou}^1$, $\text{Yuan Zhang}^1$, $\text{Mingyuan Gao}^1$
  \\
  ${}^1$ ByteDance Intelligent Creation
  \\
  \tt\small \{wanglizhen.2024, zhaogang.666, tianshu.hu, wangpengrui.chj, pengfei.wei, \\
  \tt\small zerong, zhouming.9527\}@bytedance.com
  \tt\small \{zhang.yuan09,gaomingyuan001\}@gmail.com
}
% Institution1 address\\
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% \author[1,*]{Darth Vader}
% \author[2]{Darth Sidious}
% \affil[1]{Office of the Supreme Commander of the Imperial Forces, The Galactic Empire, The Bridge, Executor}
% \affil[2]{Order of the Sith Lords, LiMerge Power Building, The Works, Coruscant}
% \affil[*]{Corresponding author: Darth Vader, darth@vader.deathstar}

\begin{document}

\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle
\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.99\textwidth]{images/teaser.png}
    \captionof{figure}{DreamActor-H1 can generate high-fidelity and photo-realistic human-product demonstration videos from human and product reference images.}
    \label{fig:teaser}
\end{center}%
}]

\let\thefootnote\relax\footnotetext{* Corresponding author \& project leader}


\begin{abstract}
In e-commerce and digital marketing, generating high-fidelity human-product demonstration videos is important for effective product presentation. However, most existing frameworks either fail to preserve the identities of both humans and products or lack an understanding of human-product spatial relationships, leading to unrealistic representations and unnatural interactions.
To address these challenges, we propose a Diffusion Transformer (DiT)-based framework. Our method simultaneously preserves human identities and product-specific details, such as logos and textures, by injecting paired human-product reference information and utilizing an additional masked cross-attention mechanism. We employ a 3D body mesh template and product bounding boxes to provide precise motion guidance, enabling intuitive alignment of hand gestures with product placements. Additionally, structured text encoding is used to incorporate category-level semantics, enhancing 3D consistency during small rotational changes across frames. Trained on a hybrid dataset with extensive data augmentation strategies, our approach outperforms state-of-the-art techniques in maintaining the identity integrity of both humans and products and generating realistic demonstration motions. Project page: https://lizhenwangt.github.io/DreamActor-H1/.
\end{abstract}

\input{sections/intro}    
\input{sections/related}
\input{sections/method}
\input{sections/exp}
\input{sections/discussion}

\begin{figure*}
  \centering
  \includegraphics[width=0.95\linewidth]{images/moreresults1.jpg}
  \caption{Our video results generated from human images and product images.}
  % \Description{}
  \label{fig-1}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=0.95\linewidth]{images/moreresults2.jpg}
  \caption{Our results generated from more human images and product images.}
  % \Description{}
  \label{fig-2}
\end{figure*}
\newpage

{
    \bibliographystyle{ieeenat_fullname}
    \bibliography{reference}
}


% WARNING: do not forget to delete the supplementary pages from your submission 
%\input{sec/X_suppl}

\end{document}
